{"cells":[{"cell_type":"code","execution_count":1,"source":["BATCH_SIZE = 32\n","MULTILABEL = True\n","USE_TEMPLATE = True\n","USE_PREPROCESSING = True"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import re\n","import numpy as np\n","import pandas as pd\n","from transformers import pipeline\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, classification_report"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["df_test = pd.read_csv('./Indonesian-Twitter-Emotion-Dataset/Twitter_Emotion_Dataset.csv')\n","X_test = df_test['tweet'].tolist()\n","y_test = df_test['label']\n","\n","label = ['kemarahan', 'ketakutan', 'kebahagiaan', 'cinta', 'kesedihan']\n","doc_count = len(X_test)\n","\n","hypotheses_template = \"{}.\"\n","if USE_TEMPLATE:\n","    hypotheses_template = \"Kalimat ini mengekspresikan {}.\"\n","\n","if USE_PREPROCESSING:\n","    X_test = [re.sub(r'\\[([A-Z]+)\\]', r'', X) for X in X_test]\n","    X_test = [re.sub(r'#(\\S+)', r'\\g<1>', X) for X in X_test]\n","    X_test = [re.sub(r'(\\.)\\1{2,}', '', X) for X in X_test]\n","    X_test = [re.sub(r'(\\W)\\1{2,}', r'\\g<1>', X) for X in X_test]\n","    X_test = [X.lower() for X in X_test]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["le = LabelEncoder()\n","y_test = le.fit_transform(y_test)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["zsl = pipeline('zero-shot-classification', device=0, model='joeddav/xlm-roberta-large-xnli', )\n","\n","y_preds = []\n","for idx_start in range(0, doc_count, BATCH_SIZE):\n","    idx_end = idx_start + BATCH_SIZE\n","    if idx_end > doc_count:\n","        idx_end = doc_count\n","    print(f'{idx_start+1}-{idx_end+1}/{doc_count}')\n","\n","    y_pred = zsl(X_test[idx_start:idx_end], label, hypotheses_template, multi_label=MULTILABEL)\n","    y_preds.extend(y_pred)\n","\n","y_preds = [label.index(y_preds[i]['labels'][0]) for i in range(doc_count)]"],"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["1-33/4401\n","33-65/4401\n","65-97/4401\n","97-129/4401\n","129-161/4401\n","161-193/4401\n","193-225/4401\n","225-257/4401\n","257-289/4401\n","289-321/4401\n","321-353/4401\n","353-385/4401\n","385-417/4401\n","417-449/4401\n","449-481/4401\n","481-513/4401\n","513-545/4401\n","545-577/4401\n","577-609/4401\n","609-641/4401\n","641-673/4401\n","673-705/4401\n","705-737/4401\n","737-769/4401\n","769-801/4401\n","801-833/4401\n","833-865/4401\n","865-897/4401\n","897-929/4401\n","929-961/4401\n","961-993/4401\n","993-1025/4401\n","1025-1057/4401\n","1057-1089/4401\n","1089-1121/4401\n","1121-1153/4401\n","1153-1185/4401\n","1185-1217/4401\n","1217-1249/4401\n","1249-1281/4401\n","1281-1313/4401\n","1313-1345/4401\n","1345-1377/4401\n","1377-1409/4401\n","1409-1441/4401\n","1441-1473/4401\n","1473-1505/4401\n","1505-1537/4401\n","1537-1569/4401\n","1569-1601/4401\n","1601-1633/4401\n","1633-1665/4401\n","1665-1697/4401\n","1697-1729/4401\n","1729-1761/4401\n","1761-1793/4401\n","1793-1825/4401\n","1825-1857/4401\n","1857-1889/4401\n","1889-1921/4401\n","1921-1953/4401\n","1953-1985/4401\n","1985-2017/4401\n","2017-2049/4401\n","2049-2081/4401\n","2081-2113/4401\n","2113-2145/4401\n","2145-2177/4401\n","2177-2209/4401\n","2209-2241/4401\n","2241-2273/4401\n","2273-2305/4401\n","2305-2337/4401\n","2337-2369/4401\n","2369-2401/4401\n","2401-2433/4401\n","2433-2465/4401\n","2465-2497/4401\n","2497-2529/4401\n","2529-2561/4401\n","2561-2593/4401\n","2593-2625/4401\n","2625-2657/4401\n","2657-2689/4401\n","2689-2721/4401\n","2721-2753/4401\n","2753-2785/4401\n","2785-2817/4401\n","2817-2849/4401\n","2849-2881/4401\n","2881-2913/4401\n","2913-2945/4401\n","2945-2977/4401\n","2977-3009/4401\n","3009-3041/4401\n","3041-3073/4401\n","3073-3105/4401\n","3105-3137/4401\n","3137-3169/4401\n","3169-3201/4401\n","3201-3233/4401\n","3233-3265/4401\n","3265-3297/4401\n","3297-3329/4401\n","3329-3361/4401\n","3361-3393/4401\n","3393-3425/4401\n","3425-3457/4401\n","3457-3489/4401\n","3489-3521/4401\n","3521-3553/4401\n","3553-3585/4401\n","3585-3617/4401\n","3617-3649/4401\n","3649-3681/4401\n","3681-3713/4401\n","3713-3745/4401\n","3745-3777/4401\n","3777-3809/4401\n","3809-3841/4401\n","3841-3873/4401\n","3873-3905/4401\n","3905-3937/4401\n","3937-3969/4401\n","3969-4001/4401\n","4001-4033/4401\n","4033-4065/4401\n","4065-4097/4401\n","4097-4129/4401\n","4129-4161/4401\n","4161-4193/4401\n","4193-4225/4401\n","4225-4257/4401\n","4257-4289/4401\n","4289-4321/4401\n","4321-4353/4401\n","4353-4385/4401\n","4385-4402/4401\n"]}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["print('F1 Score:', f1_score(y_test, y_preds, average='micro'))\n","print(classification_report(y_test, y_preds, target_names=le.classes_))"],"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.6203135650988412\n","              precision    recall  f1-score   support\n","\n","       anger       0.62      0.55      0.58      1101\n","        fear       0.52      0.81      0.63       649\n","       happy       0.77      0.60      0.68      1017\n","        love       0.62      0.81      0.70       637\n","     sadness       0.61      0.47      0.53       997\n","\n","    accuracy                           0.62      4401\n","   macro avg       0.63      0.65      0.62      4401\n","weighted avg       0.64      0.62      0.62      4401\n","\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}