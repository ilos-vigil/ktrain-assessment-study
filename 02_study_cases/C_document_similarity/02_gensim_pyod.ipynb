{"cells":[{"cell_type":"code","execution_count":1,"source":["USE_NMF = True\n","N_NEIGHBORS = 20"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import os\n","import re\n","import random\n","import pandas as pd\n","from pyod.models.lof import LOF\n","from gensim.models import LdaModel\n","from gensim.models.nmf import Nmf\n","from gensim.corpora.dictionary import Dictionary\n","from sklearn.metrics import classification_report"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["with open('./dataset/id.stopwords.02.01.2016.txt') as f:\n","    stop_words = f.read().split('\\n')\n","    stop_words_regex = r'\\b(' + '|'.join(stop_words) + r')\\b'\n","\n","def clean(doc):\n","    doc = str(doc).lower()\n","    doc = re.sub('[^A-Za-z\\ ]', ' ', doc)\n","    doc = re.sub(r'\\b\\w{0,1}\\b', '', doc)\n","    doc = re.sub(stop_words_regex, '', doc)\n","    doc = re.sub('\\s{2,}', ' ', doc)\n","    doc = doc.split(' ')\n","\n","    return doc\n","\n","def process_csv(path):\n","    df = pd.read_csv(path).astype(str)\n","    df['Article'] = df.apply(\n","        lambda d: d['Title'] + ' ' + d['Ingredients'] + ' ' + d['Steps'], axis=1\n","    )\n","    df['Article'] = df['Article'].apply(clean)\n","    df['Label'] = 1\n","    return df[['Article', 'Label']]\n","\n","def process_json(path):\n","    df = pd.read_json(path, orient='column').astype(str)\n","    df['Article'] = df['content'].apply(clean)\n","    df['Label'] = 0\n","    return df[['Article', 'Label']]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["path_train = [\n","    './dataset/recipe/dataset-ayam.csv',\n","    './dataset/recipe/dataset-ikan.csv',\n","    './dataset/recipe/dataset-kambing.csv',\n","    './dataset/recipe/dataset-sapi.csv',\n","    './dataset/recipe/dataset-udang.csv'\n","]\n","path_test = [\n","    './dataset/recipe/dataset-tahu.csv',\n","    './dataset/recipe/dataset-telur.csv',\n","    './dataset/recipe/dataset-tempe.csv',\n","    './dataset/criminality_news.json'\n","]\n","train_docs, test_docs = [], []\n","\n","for p in path_train:\n","    train_docs.append(process_csv(p))\n","X_train = pd.concat(train_docs, ignore_index=True)['Article'].tolist()\n","for p in path_test:\n","    if p[-4:] == '.csv':\n","        test_docs.append(process_csv(p))\n","    else:\n","        test_docs.append(process_json(p))\n","X_test = pd.concat(test_docs, ignore_index=True)['Article'].tolist()\n","y_test = pd.concat(test_docs, ignore_index=True)['Label'].tolist()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["dictionary = Dictionary(X_train)\n","dictionary.filter_extremes(no_below=25, no_above=0.5, keep_n=10000)\n","train_corpus = [dictionary.doc2bow(doc) for doc in X_train]\n","test_corpus = [dictionary.doc2bow(doc) for doc in X_test]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["if USE_NMF:\n","    tm = Nmf(corpus=train_corpus, id2word=dictionary, num_topics=5, w_max_iter=100)\n","else:\n","    tm = LdaModel(corpus=train_corpus, id2word=dictionary, num_topics=5, iterations=100)\n","\n","for i in range(5):\n","    print(f'Topic #{i} |', [dictionary[idx] for idx, prob in tm.get_topic_terms(i, topn=9)])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Topic #0 | ['daging', 'kecap', 'sdm', 'sapi', 'manis', 'kambing', 'sdt', 'saos', 'halus']\n","Topic #1 | ['ayam', 'bubuk', 'halus', 'kaldu', 'salam', 'santan', 'sdt', 'daging', 'rebus']\n","Topic #2 | ['udang', 'saus', 'tepung', 'sdm', 'telur', 'iris', 'bombay', 'ayam', 'masukan']\n","Topic #3 | ['ayam', 'sdm', 'tepung', 'bahan', 'sdt', 'jeruk', 'sambal', 'menit', 'adonan']\n","Topic #4 | ['cabe', 'ikan', 'jeruk', 'rawit', 'halus', 'tomat', 'ruas', 'salam', 'jahe']\n"]}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["# obtain LDA or NMF vector for train/test data\n","def get_vector(doc):\n","    doc_vector = [[i, 0.0] for i in range(5)]\n","    partial_doc_vector = tm.get_document_topics(dictionary.doc2bow(doc), minimum_probability=0)\n","    for p in partial_doc_vector:\n","        doc_vector[p[0]][1] = p[1]\n","    return [d[1] for d in doc_vector]\n","\n","train_vector = [get_vector(d) for d in X_train]\n","test_vector = [get_vector(d) for d in X_test]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["# train LOF\n","lof = LOF(n_neighbors=N_NEIGHBORS)\n","lof.fit(train_vector)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n","  metric_params=None, n_jobs=1, n_neighbors=20, novelty=True, p=2)"]},"metadata":{},"execution_count":8}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["y_pred = lof.predict(test_vector)\n","print(classification_report(y_test, y_pred, target_names=['Resep', 'Berita']))"],"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       Resep       0.65      0.89      0.75     10751\n","      Berita       0.39      0.13      0.20      5945\n","\n","    accuracy                           0.62     16696\n","   macro avg       0.52      0.51      0.47     16696\n","weighted avg       0.56      0.62      0.55     16696\n","\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}