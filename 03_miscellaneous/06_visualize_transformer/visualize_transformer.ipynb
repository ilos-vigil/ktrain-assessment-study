{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import BertConfig, BertForQuestionAnswering, BertTokenizerFast, AdamW\n","from torchviz import make_dot"]},{"cell_type":"markdown","metadata":{},"source":["First step is create the model. To reduce resource usage (or if you use CPU), reduce size of the model by reducing total encoder layer and hidden size."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BertForQuestionAnswering(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(11, 6, padding_idx=0)\n","      (position_embeddings): Embedding(10, 6)\n","      (token_type_embeddings): Embedding(2, 6)\n","      (LayerNorm): LayerNorm((6,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=6, out_features=6, bias=True)\n","              (key): Linear(in_features=6, out_features=6, bias=True)\n","              (value): Linear(in_features=6, out_features=6, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=6, out_features=6, bias=True)\n","              (LayerNorm): LayerNorm((6,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=6, out_features=24, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=24, out_features=6, bias=True)\n","            (LayerNorm): LayerNorm((6,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=6, out_features=6, bias=True)\n","              (key): Linear(in_features=6, out_features=6, bias=True)\n","              (value): Linear(in_features=6, out_features=6, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=6, out_features=6, bias=True)\n","              (LayerNorm): LayerNorm((6,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=6, out_features=24, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=24, out_features=6, bias=True)\n","            (LayerNorm): LayerNorm((6,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (qa_outputs): Linear(in_features=6, out_features=2, bias=True)\n",")\n"]}],"source":["conf = BertConfig()\n","conf.vocab_size = 11\n","conf.hidden_size = 6\n","conf.num_hidden_layers = 2\n","conf.num_attention_heads = 2\n","conf.intermediate_size = 24\n","conf.max_position_embeddings = 10\n","conf.output_hidden_states = False\n","conf.output_attentions = False\n","model = BertForQuestionAnswering(conf)\n","print(model)"]},{"cell_type":"markdown","metadata":{},"source":["Since the visualization require forward/backward process, we need to create tokenizer for the model."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [2, 7, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n","{'input_ids': [2, 9, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n","{'input_ids': [2, 7, 3, 9, 3], 'token_type_ids': [0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n"]}],"source":["tokenizer = BertTokenizerFast(\n","    vocab_file='./bert_vocab.txt',\n",")\n","\n","print(tokenizer('kentang'))\n","print(tokenizer('keju'))\n","print(tokenizer('kentang', 'keju'))"]},{"cell_type":"markdown","metadata":{},"source":["Perform forward process and keep the output."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["question='bagaimana rasanya'\n","context='kentang goreng keju enak'\n","example_input = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n","example_output = model(**example_input)\n","example_output = (example_output.start_logits, example_output.end_logits)"]},{"cell_type":"markdown","metadata":{},"source":["Now perform the visualization."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/plain":["'./bert_graphs.pdf'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dot = make_dot(example_output, params=dict(model.named_parameters()), max_attr_chars=True)\n","dot.render('./bert_graphs', format='pdf')"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
